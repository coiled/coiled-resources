{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bac00500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91da4430",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SNOWFLAKE_USER\"] = \"\"\n",
    "os.environ[\"SNOWFLAKE_PASSWORD\"] = \"\"\n",
    "os.environ[\"SNOWFLAKE_ACCOUNT\"] = \"\"\n",
    "os.environ[\"SNOWFLAKE_WAREHOUSE\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba72e47e",
   "metadata": {},
   "source": [
    "# Dask-Snowflake Integration Demo\n",
    "\n",
    "This notebook demonstrates the `dask-snowflake` integration package which supports parallel read/write from Snowflake to Python with Dask.\n",
    "\n",
    "The notebook is structured as follows:\n",
    "1. Setup remote Dask resources with Coiled\n",
    "2. Connect to Snowflake\n",
    "3. Write Data to Snowflake in Parallel\n",
    "4. Read Data from Snowflake in Parallel\n",
    "5. Use Dask to Train XGBoost on Snowflake Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a672f1a",
   "metadata": {},
   "source": [
    "## 1. Set-up Dask Resources\n",
    "\n",
    "We'll start by launching our remote Dask cluster resources using Coiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85edeee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad072bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a Coiled software environment (Docker image) that will be distributed to all workers in our Dask cluster\n",
    "# coiled.create_software_environment(\n",
    "#     name=\"snowflake\",\n",
    "#     account=\"coiled-examples\",\n",
    "#     pip=[\n",
    "#         \"dask[distributed, dataframe, diagnostics]==2021.11.2\",\n",
    "#         \"snowflake-connector-python\",\n",
    "#         \"dask-snowflake\",\n",
    "#         \"lz4\",\n",
    "#         \"xgboost\",\n",
    "#     ],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60dacb23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/rpelgrim/mambaforge/envs/snowflake/lib/python3.9/site-packages/rich/live.py:221: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/rpelgrim/mambaforge/envs/snowflake/lib/python3.9/site-packages/rich/live.py:221: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using existing cluster: <span style=\"color: #008000; text-decoration-color: #008000\">'coiled-snowflake'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Using existing cluster: \u001b[32m'coiled-snowflake'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# spin up Coiled cluster\n",
    "cluster = coiled.Cluster(\n",
    "    name=\"coiled-snowflake\",\n",
    "    software=\"coiled-examples/snowflake\",\n",
    "    n_workers=20,\n",
    "    shutdown_on_close=False,\n",
    "    scheduler_options={'idle_timeout':'2 hours'},\n",
    "    backend_options={'spot':'True'},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "992efbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rpelgrim/mambaforge/envs/snowflake/lib/python3.9/site-packages/distributed/client.py:1131: VersionMismatchWarning: Mismatched versions found\n",
      "\n",
      "+---------+--------+-----------+---------+\n",
      "| Package | client | scheduler | workers |\n",
      "+---------+--------+-----------+---------+\n",
      "| pandas  | 1.3.4  | 1.3.5     | 1.3.5   |\n",
      "+---------+--------+-----------+---------+\n",
      "  warnings.warn(version_module.VersionMismatchWarning(msg[0][\"warning\"]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'http://34.205.89.212:8787'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect cluster to Dask\n",
    "from dask.distributed import Client\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52b782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd68696d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0335966",
   "metadata": {},
   "source": [
    "## 2. Connect to Snowflake\n",
    "Let's now connect our Python session to Snowflake using Snowflake's connector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9efbfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import snowflake.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef12a398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Snowflake Python connector\n",
    "ctx = snowflake.connector.connect(\n",
    "    user=os.environ[\"SNOWFLAKE_USER\"],\n",
    "    password=os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "    account=os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e98bf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run sample code to test connection\n",
    "cs = ctx.cursor()\n",
    "\n",
    "schema = \"TPCDS_SF100TCL\"\n",
    "table = \"CALL_CENTER\"\n",
    "\n",
    "cs.execute(\"USE SNOWFLAKE_SAMPLE_DATA\")\n",
    "cs.execute(\"SELECT * FROM \" + schema + \".\" + table)\n",
    "\n",
    "one_row = str(cs.fetchone())\n",
    "\n",
    "print(one_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2abc02c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efc9db2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7c5e4b1",
   "metadata": {},
   "source": [
    "## 3. Parallel Write to Snowflake\n",
    "\n",
    "Now that we have launched our remote compute resources and tested our connection to Snowflake, let's generate some synthetic data with Dask and then write to a Snowflake database in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff7aef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbda522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate synthetic timeseries data\n",
    "ddf = dask.datasets.timeseries(\n",
    "    start=\"2021-01-01\",\n",
    "    end=\"2021-03-31\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8f19d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7f734f7ebc70>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create warehouse and database\n",
    "cs.execute(\"CREATE WAREHOUSE IF NOT EXISTS dask_snowflake_wh\")\n",
    "cs.execute(\"CREATE DATABASE IF NOT EXISTS dask_snowflake_db\")\n",
    "cs.execute(\"USE DATABASE dask_snowflake_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d09336ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_snowflake import to_snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70368fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_kwargs = {\n",
    "    \"user\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "    \"password\": os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "    \"account\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "    \"warehouse\": os.environ[\"SNOWFLAKE_WAREHOUSE\"],\n",
    "    \"database\": \"dask_snowflake_db\",\n",
    "    \"schema\": \"PUBLIC\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1151bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/mambaforge/envs/snowflake/lib/python3.9/site-packages/snowflake/sqlalchemy/snowdialect.py:206: SAWarning: Dialect snowflake:snowflake will not make use of SQL compilation caching as it does not set the 'supports_statement_cache' attribute to ``True``.  This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions.  Dialect maintainers should seek to set this attribute to True after appropriate development and testing for SQLAlchemy 1.4 caching support.   Alternatively, this attribute may be set to False which will disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf)\n",
      "  results = connection.execute(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 936 ms, sys: 44.5 ms, total: 981 ms\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# write Dask dataframe to Snowflake in parallel\n",
    "to_snowflake(\n",
    "    ddf,\n",
    "    name=\"dask_snowflake_table\",\n",
    "    connection_kwargs=connection_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865a5b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d93044e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea031b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1034a437",
   "metadata": {},
   "source": [
    "## 4. Parallel Read from Snowflake\n",
    "We can now read this data back into our Python session in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4dd892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_snowflake import read_snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aba8e41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ID      NAME         X         Y\n",
      "0  1029   Norbert  0.652481 -0.937071\n",
      "1   992     Laura  0.063575  0.909713\n",
      "2  1002  Patricia  0.593139 -0.653950\n",
      "3  1036       Dan -0.340827  0.678265\n",
      "4  1042     Frank  0.052302  0.782666\n",
      "CPU times: user 262 ms, sys: 7.89 ms, total: 270 ms\n",
      "Wall time: 4.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# read data from snowflake into a Dask dataframe\n",
    "snowflake_data = read_snowflake(\n",
    "    query=\"\"\"\n",
    "      SELECT *\n",
    "      FROM dask_snowflake_table;\n",
    "   \"\"\",\n",
    "    connection_kwargs=connection_kwargs,\n",
    ")\n",
    "\n",
    "print(snowflake_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "767517aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>NAME</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=74</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>int16</td>\n",
       "      <td>object</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: read-snowflake, 74 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                   ID    NAME        X        Y\n",
       "npartitions=74                                 \n",
       "                int16  object  float64  float64\n",
       "                  ...     ...      ...      ...\n",
       "...               ...     ...      ...      ...\n",
       "                  ...     ...      ...      ...\n",
       "                  ...     ...      ...      ...\n",
       "Dask Name: read-snowflake, 74 tasks"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowflake_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17e4f14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.50774355061549e-05\n"
     ]
    }
   ],
   "source": [
    "# perform computation over Snowflake data with Dask\n",
    "result = snowflake_data.X.mean()\n",
    "print(result.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc92f35b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660d43f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117f40ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce98e5a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "795ce7c5",
   "metadata": {},
   "source": [
    "## 5. Machine Learning\n",
    "After loading data into our Python session from Snowflake, we can use Python what it's good for: things like free-form, iterative exploratory analyses and complex Machine Learning models.\n",
    "\n",
    "Let's read in some data from Snowflake using the `dask-snowflake` connector and then train an XGBoost ML model on that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fce33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define schema and query\n",
    "SCHEMA = \"SNOWFLAKE_SAMPLE_DATA.TPCH_SF100\"\n",
    "\n",
    "example_query=f\"\"\"\n",
    "SELECT\n",
    "    \n",
    "    C_CUSTKEY,\n",
    "    C_NAME,\n",
    "    SUM(L_QUANTITY) AS sum_qty,\n",
    "    SUM(PS_AVAILQTY) AS sum_avail_qty,\n",
    "    MAX(P_RETAILPRICE) AS max_retail_price\n",
    "    \n",
    "    FROM {SCHEMA}.CUSTOMER\n",
    "    \n",
    "        JOIN {SCHEMA}.ORDERS\n",
    "            ON C_CUSTKEY = O_CUSTKEY\n",
    "            \n",
    "            JOIN {SCHEMA}.LINEITEM\n",
    "                ON L_ORDERKEY = O_ORDERKEY\n",
    "                \n",
    "                JOIN {SCHEMA}.PART\n",
    "                    ON P_PARTKEY = L_PARTKEY\n",
    "                    \n",
    "                    JOIN {SCHEMA}.PARTSUPP\n",
    "                        ON P_PARTKEY = PS_PARTKEY\n",
    "    \n",
    "    WHERE PS_SUPPLYCOST > 10\n",
    "\n",
    "GROUP BY C_CUSTKEY, C_NAME\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f759ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set connection parameters\n",
    "connection_kwargs = {\n",
    "    \"user\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "    \"password\": os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "    \"account\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "    \"warehouse\": os.environ[\"SNOWFLAKE_WAREHOUSE\"],\n",
    "    \"database\": \"SNOWFLAKE_SAMPLE_DATA\",\n",
    "    \"schema\": \"TPCH_SF100\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8951e745",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# read in data from snowflake\n",
    "ddf = read_snowflake(\n",
    "    query=example_query,\n",
    "    connection_kwargs=connection_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec717384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d183d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define predictor and target features\n",
    "X = ddf[['SUM_AVAIL_QTY', 'MAX_RETAIL_PRICE']]\n",
    "y = ddf.SUM_QTY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a00dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Dask DMatrix\n",
    "dtrain = xgb.dask.DaskDMatrix(client, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063e43d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train XGBoost with Dask\n",
    "output = xgb.dask.train(\n",
    "    client,\n",
    "    {\n",
    "        'verbosity': 2,\n",
    "        'tree_method': 'hist',\n",
    "        'objective': 'reg:squarederror'\n",
    "    },\n",
    "    dtrain,\n",
    "    num_boost_round=10,\n",
    "    evals=[(dtrain, 'train')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe2a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_pred = xgb.dask.predict(client, output[\"booster\"], X)\n",
    "y_pred.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc02624",
   "metadata": {},
   "source": [
    "For more details on how to use distributed XGBoost training with Dask, see [this blog post](https://coiled.io/blog/dask-xgboost-python-example/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d85cc9",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5909007a",
   "metadata": {},
   "source": [
    "This notebook has demonstrated:\n",
    "1. How to use the `dask-snowflake` connector for fast, parallel data transfer between Snowflake and Python\n",
    "2. How to use Dask to continue manipulating the Snowflake data in a Python session, performing iterative EDA and/or machine learning tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
