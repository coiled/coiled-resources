{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34e3f87f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/dask/dask/main/docs/source/images/dask_horizontal_no_pad.svg\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\\\" />\n",
    "     \n",
    "This notebook was inspired by the materials from: \n",
    "\n",
    "- https://github.com/coiled/pydata-global-dask/\n",
    "\n",
    "# Dask DataFrame\n",
    "\n",
    "As we learned on our Dask Delayed notebook we can parallelize functions using `dask.delayed`, but luckily for us we have multiple Dask collections that already include a parallel version of multiple functions. In this notebook we will learn about the [Dask DataFrame](https://docs.dask.org/en/latest/dataframe.html), a Pandas DataFrame interface that will automatically build parallel computations for tabular data.\n",
    "\n",
    "## When to use Dask DataFrames\n",
    "\n",
    "Pandas is great for tabular datasets that fit in memory. If your data fits in memory then you should use Pandas. **Dask becomes useful when the dataset you want to analyze is larger than your machine's RAM** where you would normally run into `MemoryError`s.\n",
    "\n",
    "```python\n",
    "    MemoryError:  ...\n",
    "```\n",
    "\n",
    "During this tutorial, the example NYC dataset we're working with is only about 200MB so that you can download it in a reasonable time and exercises finish quickly, but Dask Dataframes will scale to datasets much larger than the memory on your local machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37867c1",
   "metadata": {},
   "source": [
    "# Getting started with Dask DataFrames\n",
    "\n",
    "Let's use Dask DataFrame's to explore our NYC flight dataset. Dask's `read_csv` function will automatically example wildcard characters like `\"*\"` which can, for example, be used to load an entire directory of CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50398620",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../prep_data.py -d flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4169babd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "files = os.path.join('../data', 'nycflights', '*.csv')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4c4e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b879a54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_csv(files,\n",
    "                 parse_dates={'Date': [0, 1, 2]},\n",
    "                 dtype={\"TailNum\": str,\n",
    "                        \"CRSElapsedTime\": float,\n",
    "                        \"Cancelled\": bool})\n",
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2a3583",
   "metadata": {},
   "source": [
    "Notice that the representation of the dataframe object contains no data - Dask has just done enough to read the start of the first file, and infer the column names and dtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefc5d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4495ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0b42ec",
   "metadata": {},
   "source": [
    "Dask DataFrames have an `.npartitions` attribute which tells you how many Pandas DataFrames make up a Dask DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c685c721",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.npartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d794db-0ecb-498e-bb5e-91b940cb554d",
   "metadata": {},
   "source": [
    "## The Dask DataFrame data model\n",
    "\n",
    "For the most part, a Dask DataFrame feels like a Pandas DataFrame. However, internally a Dask DataFrame is composed of many Pandas DataFrames (see the image below). \n",
    "\n",
    "<img src=\"http://dask.pydata.org/en/latest/_images/dask-dataframe.svg\" width=\"30%\">\n",
    "\n",
    "Dask DataFrames are partitioned along their index into different **partitions** where each parition is a normal Pandas DataFrame. These Pandas objects may live on disk or on other machines.\n",
    "\n",
    "Dask DataFrames implement a well-used portion of the Pandas API which are backed by blocked algorithms that allow for parallel and out-of-core computation. For more details about what Pandas operations are implemented in Dask, check the [Dask DataFrame API documentation](http://docs.dask.org/en/latest/dataframe-api.html). \n",
    "\n",
    "For many purposes Dask DataFrames can serve as drop-in replacements for Pandas DataFrames. Much like the Dask Delayed interface, Dask DataFrames are lazily evaluated. You can use use the DataFrame API to automatically build up a task graph representing complex computations and then call `compute()` to to evaluate the graph in parallel. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b53d05-8314-497b-bede-9dba1c8b422f",
   "metadata": {},
   "source": [
    "## Task Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306be7d1",
   "metadata": {},
   "source": [
    "Let's look at the task graph for our Dask DataFrame to get a sense for where these partitions are coming from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524b8739",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c23c794",
   "metadata": {},
   "source": [
    "Each partition in our Dask DataFrame is the result of calling Pandas' `read_csv` on an input CSV file in our dataset.\n",
    "\n",
    "We can view the start of the data with `df.head()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdf1c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fb66a7",
   "metadata": {},
   "source": [
    "`.head()` triggers a computation to show the first 10 rows of the DataFrame. As you might have noticed we can perform computations on Dask DataFrames using the single machine scheduler, but we recommend you use a distributed cluster so you can see the dashboard in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38772f5f",
   "metadata": {},
   "source": [
    "# Computations with Dask DataFrames\n",
    "\n",
    "Since Dask DataFrames implement a Pandas-like API, we can write familiar looking Pandas code using our Dask DataFrames. For example, let's compute the largest flight departure delay.\n",
    "\n",
    "In Pandas we would do this by iterating over each file to find the individual maximums and then find the final maximum over the individual maximums.\n",
    "\n",
    "```python\n",
    "maxes = []\n",
    "for fn in filenames:\n",
    "    df = pd.read_csv(fn)\n",
    "    maxes.append(df[\"DepDelay\"].max())\n",
    "\n",
    "final_max = max(maxes)\n",
    "```\n",
    "\n",
    "We could wrap the `pd.read_csv` with `dask.delayed` so that it runs in parallel. But, we're still dealing with loops, intermediate results (one per file) and the final reduction (max of the intermediate maxes)Thankfully, we have can do this with Dask DataFrames using Pandas-like code by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc85aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_delay = ddf[\"DepDelay\"].max()\n",
    "max_delay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c331401e",
   "metadata": {},
   "source": [
    "The above cell looks exactly like what we would do using Pandas and constructs a task graph that we can compute in parallel. Let's look at the task graph to get a feel for how Dask's blocked algorithms work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274685a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_delay.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15fa313",
   "metadata": {},
   "source": [
    "Some things to note:\n",
    "\n",
    "1.  Up until this point everything is lazy. To evaluate the result for `max_delay`, call its `compute()` method:\n",
    "2.  Dask will delete intermediate results (like the full pandas DataFrame for each file) as soon as possible.\n",
    "    -  This lets us handle datasets that are larger than memory\n",
    "    -  This means that repeated computations will have to load all of the data in each time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2857454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "max_delay.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0732977",
   "metadata": {},
   "source": [
    "## Pandas' like computations\n",
    "\n",
    "Let's see couple of examples on how the API for Dask DataFrames is the same than Pandas. If you are comfortable with Pandas, the following operations will look very familiar, except we will need to add the `compute()` to get the results wanted.\n",
    "\n",
    "### Example 1: Total of non-canelled flights taken\n",
    "\n",
    "Notice that there is a column in our DataFrame called `\"Cancelled\"` that is a boolean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9582579",
   "metadata": {},
   "outputs": [],
   "source": [
    "(~ddf[\"Cancelled\"]).sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6341c992",
   "metadata": {},
   "source": [
    "### Example 2: Total of non-canceled flights taken by airport\n",
    "\n",
    "We should select the non-canceled flights, use the operation `groupby` on the `\"Origin\"` column and finally use `count` to get the detailed per airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4b6b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf[~ddf[\"Cancelled\"]].groupby(\"Origin\")[\"Origin\"].count().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7450550",
   "metadata": {},
   "source": [
    "### Exercise 1: What is the average departure delay from each airport?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e0ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution\n",
    "ddf.groupby(\"Origin\")[\"DepDelay\"].mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d983dcf",
   "metadata": {},
   "source": [
    "### Exercise 2: What day of the week has the worst average departure delay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3495711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution\n",
    "ddf.groupby(\"DayOfWeek\")[\"DepDelay\"].mean().idxmax().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16175c0",
   "metadata": {},
   "source": [
    "## Performance tip: Share intermediate results\n",
    "\n",
    "\n",
    "In the examples and exercises above, we sometimes perform the same operation more than once (e.g. `read_csv`). For most operations, Dask DataFrames hashes the arguments, allowing duplicate computations to be shared, and only computed once.\n",
    "\n",
    "For example, let's compute the mean and standard deviation for departure delay of all non-canceled flights. Since Dask operations are lazy, those values aren't the final results until we `compute` them. They're just the recipe required to get the result.\n",
    "\n",
    "If we compute them with two calls to compute, there is no sharing of intermediate computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0fca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_cancelled = ddf[~ddf[\"Cancelled\"]]\n",
    "mean_delay = non_cancelled[\"DepDelay\"].mean()\n",
    "std_delay = non_cancelled[\"DepDelay\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ad2345",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mean_delay_result = mean_delay.compute()\n",
    "std_delay_result = std_delay.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15616d97",
   "metadata": {},
   "source": [
    "Now, let's see how long it takes if we try computing `mean_delay` and `std_delay` with a single `compute()` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d83eabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3151d164",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mean_delay_res, std_delay_res = dask.compute(mean_delay, std_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e74a1",
   "metadata": {},
   "source": [
    "Using `dask.compute` takes roughly 1/2 the time. This is because the task graphs for both results are merged when calling `dask.compute`, allowing shared operations (like `read_csv`) to only be done once instead of twice. In particular, using `dask.compute` only does the following once:\n",
    "\n",
    "- The calls to `read_csv`\n",
    "- The filter (`df[~df[\"Cancelled\"]]`)\n",
    "- The `\"DepDelay\"` column indexing\n",
    "- Some of the necessary reductions (`sum`, `count`)\n",
    "\n",
    "To see what the merged task graphs between multiple results look like (and what's shared), you can use the `dask.visualize` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f82c607",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(mean_delay, std_delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb92571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912556cc-dbaf-48f1-ae14-bcb4d2bb4aa1",
   "metadata": {},
   "source": [
    "## The Dask Distributed Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79d4513-bcbe-4917-afc0-2915b1262d5a",
   "metadata": {},
   "source": [
    "There are a few different Dask schedulers, but basically when starting out on your Dask journey we recommend using only the **distributed scheduler**. This scheduler offers more features and diagnostics. You can think of the distributed scheduler as an \"advanced scheduler\". \n",
    "\n",
    "The distributed scheduler can be used in a cluster as well as locally. Deploying a remote Dask cluster involves additional setup that you can read more about on the Dask [setup documentation](https://docs.dask.org/en/latest/setup.html). Alternatively, you can use [Coiled](https://docs.coiled.io/user_guide/index.html#what-is-coiled) which provides a cluster-as-a-service functionality to provision hosted Dask clusters on demand, and you can try it for free.  \n",
    "\n",
    "For now, we will set up the scheduler locally. To set up the distributed scheduler locally we need to create a `Client` object, which will let you interact with the \"cluster\" (local threads or processes on your machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed49d94-4db3-4904-883c-683a315c4be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import Client\n",
    "\n",
    "client = Client() #shorthand for creating a 'local cluster' of all your machine's cores\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d1f5f8-3b1b-4684-bc18-427295f8ab5b",
   "metadata": {},
   "source": [
    "## The Dask Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4392d18a-0289-4e8c-bc5d-f22cb144def1",
   "metadata": {},
   "source": [
    "When we create a distributed scheduler Client, it registers itself as the default Dask scheduler. From now on, all `.compute()` calls will start using the distributed scheduler unless otherwise is specified.\n",
    "\n",
    "The distributed scheduler has many features that you can learn more about in the Dask distributed documentation but a nice feature to explore is diagnostic the Dashboard. We will be taking a look at the dashboard as we perform computations but for a brief overview of the main components of the dashboard you can check the Dask documentation on diagnosing performance.\n",
    "\n",
    "If you click on the link of the dashboard on the cell above and run the computation we did before you will see now some action happening on the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b98fee9-365c-4814-8b51-8a3f7a01fafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf.groupby(\"DayOfWeek\")[\"DepDelay\"].mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41b5b91-e2e4-420a-bb97-c82f72d0a1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f08a35",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extra resources\n",
    "\n",
    "- Explore applying custom code to Dask DataFrames: [Dask Tutorial DataFrames](https://github.com/dask/dask-tutorial/blob/main/04_dataframe.ipynb)\n",
    "- [Dask DataFrame documentation](https://docs.dask.org/en/latest/dataframe.html)\n",
    "- [Dask DataFrame examples](https://examples.dask.org/dataframe.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coiled-base",
   "language": "python",
   "name": "coiled-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
